import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re

def scan_static_site(url):
    """
    Perform basic security checks on a static website.
    """
    print(f"Scanning {url}...\n")
    try:
        # Get the HTML content of the page
        response = requests.get(url)
        response.raise_for_status()
        content = response.text
        
        # Check for common issues
        check_insecure_headers(response.headers)
        check_sensitive_info(content)
        find_links(url, content)
    except requests.exceptions.RequestException as e:
        print(f"Error accessing {url}: {e}")

def check_insecure_headers(headers):
    """
    Check for insecure HTTP headers.
    """
    print("Checking HTTP headers...\n")
    if 'X-Frame-Options' not in headers:
        print("Warning: Missing 'X-Frame-Options' header (protection against clickjacking).")
    if 'Content-Security-Policy' not in headers:
        print("Warning: Missing 'Content-Security-Policy' header (protection against XSS).")
    if 'Strict-Transport-Security' not in headers:
        print("Warning: Missing 'Strict-Transport-Security' header (enforce HTTPS).")
    print()

def check_sensitive_info(content):
    """
    Check for exposed sensitive information in the HTML.
    """
    print("Searching for sensitive information in the HTML...\n")
    sensitive_patterns = {
        "API Keys": r"(api_key|apikey|api-key)[\s]*[:=][\s]*['\"]?[a-zA-Z0-9_\-]{10,}",
        "Email Addresses": r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
        "Sensitive Comments": r"<!--.*?-->",
    }

    for label, pattern in sensitive_patterns.items():
        matches = re.findall(pattern, content, re.IGNORECASE)
        if matches:
            print(f"Found potential {label}:")
            for match in matches:
                print(f"  {match}")
    print()

def find_links(base_url, content):
    """
    Extract and analyze all links on the page.
    """
    print("Extracting and analyzing links...\n")
    soup = BeautifulSoup(content, 'html.parser')
    links = soup.find_all('a', href=True)

    for link in links:
        href = link['href']
        full_url = urljoin(base_url, href)
        print(f"Found link: {full_url}")
        # Check if the link is external or uses insecure HTTP
        if full_url.startswith("http://"):
            print(f"  Warning: Insecure HTTP link found: {full_url}")
    print()

if __name__ == "__main__":
    # Run static site's URL
    website_url = "https://sea-turtle-app-rgr24.ondigitalocean.app/"
    scan_static_site(website_url)
